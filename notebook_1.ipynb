{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Rail Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to create, interact with and render our first railway systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In a nutshell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flatland\n",
    "!git clone https://gitlab.aicrowd.com/flatland/flatland.git/ --branch 223_UpdateEditor_55_notebooks\n",
    "%cd flatland\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import rail_from_manual_specifications_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fixed railway from a 2D array of (tile type, rotation) tuples\n",
    "specs = [[(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "         [(0, 0), (0, 0), (0, 0), (0, 0), (7, 0), (0, 0)],\n",
    "         [(7, 270), (1, 90), (1, 90), (1, 90), (2, 90), (7, 90)],\n",
    "         [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]]\n",
    "\n",
    "rail_shape = np.array(specs).shape\n",
    "\n",
    "fixed_env = RailEnv(width=rail_shape[1],\n",
    "              height=rail_shape[0],\n",
    "              rail_generator=rail_from_manual_specifications_generator(specs),\n",
    "              number_of_agents=1\n",
    "              )\n",
    "\n",
    "# Call reset() to initialize the env\n",
    "observation = fixed_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will study observations in details in the next notebook.\n",
    "\n",
    "# As a first approach: by default, for each agent, the observation contains 3 arrays\n",
    "agent_handle = 0\n",
    "print('Agent {} observations:'.format(agent_handle))\n",
    "agent_obs = observation[0][agent_handle]\n",
    "\n",
    "print('Transition map')\n",
    "print(agent_obs[0])\n",
    "print()\n",
    "\n",
    "print('State of all agents in the environment')\n",
    "print(agent_obs[1])\n",
    "print()\n",
    "\n",
    "print('Position and targets of all the agents')\n",
    "print(agent_obs[2])\n",
    "print()\n",
    "\n",
    "# Additional information from the observation\n",
    "print('Additional information')\n",
    "print(observation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatland.envs.rail_env import RailEnvActions\n",
    "\n",
    "# Calling step() returns dictionaries\n",
    "next_obs, all_rewards, done, _ = fixed_env.step({agent_handle: RailEnvActions.MOVE_FORWARD})\n",
    "\n",
    "print('Rewards for each agent:')\n",
    "print(all_rewards)\n",
    "\n",
    "print('Done for each agent:')\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from flatland.utils.rendertools import RenderTool\n",
    "\n",
    "# Render the environment\n",
    "def render_env(env):\n",
    "    env_renderer = RenderTool(env, gl=\"PILSVG\")\n",
    "    env_renderer.render_env()\n",
    "\n",
    "    image = env_renderer.get_image()\n",
    "    pil_image = PIL.Image.fromarray(image)\n",
    "    display(pil_image)\n",
    "\n",
    "render_env(fixed_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatland.envs.rail_generators import random_rail_generator\n",
    "\n",
    "# Random rail networks can be built from a list of probability per cell type\n",
    "transition_probability = [1.0,  # empty cell - Case 0\n",
    "                          1.0,  # Case 1 - straight\n",
    "                          1.0,  # Case 2 - simple switch\n",
    "                          0.3,  # Case 3 - diamond drossing\n",
    "                          0.5,  # Case 4 - single slip\n",
    "                          0.5,  # Case 5 - double slip\n",
    "                          0.2,  # Case 6 - symmetrical\n",
    "                          0.0,  # Case 7 - dead end\n",
    "                          0.2,  # Case 8 - turn left\n",
    "                          0.2,  # Case 9 - turn right\n",
    "                          1.0]  # Case 10 - mirrored switch\n",
    "\n",
    "random_env = RailEnv(width=15,\n",
    "              height=15,\n",
    "              rail_generator=random_rail_generator(\n",
    "                        cell_type_relative_proportion=transition_probability\n",
    "                        ),\n",
    "              number_of_agents=1)\n",
    "\n",
    "random_env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_env(random_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create and \"train\" a random agent\n",
    "class RandomAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, memories):\n",
    "        # This is where the agent would learn\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def run_episode(env):\n",
    "    agent = RandomAgent(218, 4)\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    score = 0\n",
    "    action_dict = dict()\n",
    "\n",
    "    for step in range(15):\n",
    "        clear_output(wait=True)\n",
    "        render_env(env)\n",
    "        \n",
    "        for agent_handle in range(env.get_num_agents()):\n",
    "            action = agent.act(obs[agent_handle])\n",
    "            action_dict.update({agent_handle: action})\n",
    "            print('Step {}: agent {} takes action {}'.format(step, agent_handle, RailEnvActions.to_char(action)))\n",
    "\n",
    "        next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "\n",
    "        for a in range(env.get_num_agents()):\n",
    "            agent.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "            score += all_rewards[a]\n",
    "\n",
    "        print('Total return = {}'.format(score))\n",
    "        \n",
    "        if done['__all__']:\n",
    "            print('All done!')\n",
    "            break\n",
    "\n",
    "# Run an episode in the fixed environment\n",
    "run_episode(fixed_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one will take longer!\n",
    "run_episode(random_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's it!** You've created and rendered your first rail environments, and ran a random agent in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In more details...\n",
    "\n",
    "Let's dive into more details about some points mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed rail network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to generate a rail network. The simpler one is to describe it explicitely, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = [[(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "         [(0, 0), (0, 0), (0, 0), (0, 0), (7, 0), (0, 0)],\n",
    "         [(7, 270), (1, 90), (1, 90), (1, 90), (2, 90), (7, 90)],\n",
    "         [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`specs` is a 2-dimensional array of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rail_shape = np.array(specs).shape\n",
    "rail_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `specs` array represent a 4 by 6 2D grid of tuples. In each tuple, the first element represent the **cell type**, and the second the **rotation** of the cell (0, 90, 180 or 270 degrees clockwise).\n",
    "\n",
    "The following image gives an overview of the eight basic types. These can be rotated in steps of 45° and mirrored along the North-South of East-West axis.\n",
    "\n",
    "![cell types](https://drive.google.com/uc?export=view&id=164iowmfRQ9O34hquxLhO2xxt49NE473P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the list of transitions as defined in flatland.core.grid.rail_env_grid.RailEnvTransitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "transition_list = [int('0000000000000000', 2),  # empty cell - Case 0\n",
    "                   int('1000000000100000', 2),  # Case 1 - straight\n",
    "                   int('1001001000100000', 2),  # Case 2 - simple switch\n",
    "                   int('1000010000100001', 2),  # Case 3 - diamond drossing\n",
    "                   int('1001011000100001', 2),  # Case 4 - single slip\n",
    "                   int('1100110000110011', 2),  # Case 5 - double slip\n",
    "                   int('0101001000000010', 2),  # Case 6 - symmetrical\n",
    "                   int('0010000000000000', 2),  # Case 7 - dead end\n",
    "                   int('0100000000000010', 2),  # Case 1b (8)  - simple turn right\n",
    "                   int('0001001000000000', 2),  # Case 1c (9)  - simple turn left\n",
    "                   int('1100000000100010', 2)]  # Case 2b (10) - simple switch mirrored\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RailEnv` accepts a `rail_generator` parameter, which is used to generate the rail networks. When creating fixed networks, we use `rail_from_manual_specifications_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RailEnv(width=rail_shape[1],\n",
    "              height=rail_shape[0],\n",
    "              rail_generator=rail_from_manual_specifications_generator(specs),\n",
    "              number_of_agents=1\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to `reset()` is necessary to fully initialize the environment. As usual with gym-like environment, this returns the initial observation as a results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, the environment provides very complete observations by default. You typically won't use this object as-is. One of the main objectives of the Flatland challenge is to **find suitable observations** to solve the task at hand. We will see how this works in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an agent\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RailEnv` is a gym environment. However, as opposed to most environments, it is targeted at multi-agents experiments. For this purpose, it is derived from RLLib's `MultiAgentEnv` ([more details about it here](https://ray.readthedocs.io/en/latest/rllib-env.html)).\n",
    "\n",
    "The environment can be run by supplying the environment step function with a **dictionary** of actions, whose keys are agents’ handles and the corresponding values are the selected actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = fixed_env\n",
    "\n",
    "obs = env.reset()\n",
    "agent = RandomAgent(218, 4)\n",
    "action_dict = dict()\n",
    "\n",
    "for handle in env.get_agent_handles():\n",
    "    action = agent.act(obs[handle])\n",
    "    action_dict.update({handle: action})\n",
    "    \n",
    "for (handle, action) in action_dict.items():\n",
    "    print('Agent {} will perform action {} ({})'.format(handle, action, RailEnvActions.to_char(action)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we use `env.get_agent_handles()` to enumarate through the handles, and `RailEnvActions.to_char` to get a symbol representing the agent's direction (**B**ackward, **F**orward, **L**eft, **R**ight, **S**top): "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
